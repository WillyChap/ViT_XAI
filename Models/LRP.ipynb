{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b03f59d5-43b2-4634-a082-0b4774a12244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import os\n",
    "from baselines.ViT.ViT_explanation_generator import LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05578e97-91e7-4af0-a2a4-ce86aa3e8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ADD THIS ######\n",
    "# https://github.com/hila-chefer/Transformer-Explainability/blob/main/baselines/ViT/ViT_LRP.py\n",
    "\n",
    "def relprop(self, cam=None,method=\"transformer_attribution\", is_ablation=False, start_layer=0, **kwargs):\n",
    "        # print(kwargs)\n",
    "        # print(\"conservation 1\", cam.sum())\n",
    "        cam = self.head.relprop(cam, **kwargs)\n",
    "        cam = cam.unsqueeze(1)\n",
    "        cam = self.pool.relprop(cam, **kwargs)\n",
    "        cam = self.norm.relprop(cam, **kwargs)\n",
    "        for blk in reversed(self.blocks):\n",
    "            cam = blk.relprop(cam, **kwargs)\n",
    "\n",
    "        # print(\"conservation 2\", cam.sum())\n",
    "        # print(\"min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e066c1-1c5f-4901-8039-9d64d258d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device available : True\n",
      "device count:  1\n",
      "current device:  0\n",
      "device name:  NVIDIA A100-PCIE-40GB\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Memory Usage:\n",
      "Allocated: 0.4 GB\n",
      "Cached:    0.4 GB\n",
      "using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "def set_gpu(gpu_id):\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "gpu_id=0\n",
    "\n",
    "if gpu_id >= 0:\n",
    "    device = \"cuda\"\n",
    "    set_gpu(gpu_id)\n",
    "    print('device available :', torch.cuda.is_available())\n",
    "    print('device count: ', torch.cuda.device_count())\n",
    "    print('current device: ',torch.cuda.current_device())\n",
    "    print('device name: ',torch.cuda.get_device_name())\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "print('using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35d0f41f-8978-4c58-8335-ca2fe442cad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'saved_models/benchmark/model001.pt'\n",
    "model = torch.load(PATH).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301cf43f-0d3c-47ea-a131-cadcc613425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_fil = '/glade/work/wchapman/ViT_XAI/Data_Staging/Mamalakis_Dataset/scp_synth_exm_data.nc'\n",
    "Data_ = xr.open_dataset(dir_fil)\n",
    "Data_train = np.expand_dims(Data_['SSTrand'].isel(time=slice(0,900000)),1)\n",
    "Y_train = np.array(Data_['y'].isel(time=slice(0,900000)))\n",
    "\n",
    "Data_val = np.expand_dims(Data_['SSTrand'].isel(time=slice(950000,1000000)),1)\n",
    "Y_val = np.array(Data_['y'].isel(time=slice(950000,1000000)))\n",
    "\n",
    "Data_test = np.expand_dims(Data_['SSTrand'].isel(time=slice(900000,950000)),1)\n",
    "Y_test = np.array(Data_['y'].isel(time=slice(900000,950000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85e10ef9-0e00-433d-b8a0-e0f287aa8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.nan_to_num(torch.tensor(Data_test[0], dtype=torch.float32))\n",
    "# model(x.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cd0cbf5-08fc-483c-adee-813f87c4e87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleViT' object has no attribute 'relprop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m attribution_generator \u001b[38;5;241m=\u001b[39m LRP(model)\n\u001b[0;32m----> 2\u001b[0m transformer_attribution \u001b[38;5;241m=\u001b[39m \u001b[43mattribution_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_LRP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer_attribution\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/glade/work/kjmayer/research/catalyst/ViT_XAI/Models/baselines/ViT/ViT_explanation_generator.py:40\u001b[0m, in \u001b[0;36mLRP.generate_LRP\u001b[0;34m(self, input, index, method, is_ablation, start_layer)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m one_hot\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelprop\u001b[49m(torch\u001b[38;5;241m.\u001b[39mtensor(one_hot_vector)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), method\u001b[38;5;241m=\u001b[39mmethod, is_ablation\u001b[38;5;241m=\u001b[39mis_ablation,\n\u001b[1;32m     41\u001b[0m                           start_layer\u001b[38;5;241m=\u001b[39mstart_layer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/glade/work/wchapman/miniconda3.1/envs/MLWPS/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleViT' object has no attribute 'relprop'"
     ]
    }
   ],
   "source": [
    "attribution_generator = LRP(model)\n",
    "transformer_attribution = attribution_generator.generate_LRP(x.unsqueeze(0).to(device), method=\"transformer_attribution\", index=0).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9bbf96-5347-4539-95df-32799c08aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visualization(original_image, class_index=None):\n",
    "    transformer_attribution = attribution_generator.generate_LRP(original_image.unsqueeze(0).cuda(), method=\"transformer_attribution\", index=class_index).detach()\n",
    "    transformer_attribution = transformer_attribution.reshape(1, 1, 36, 18)\n",
    "    # transformer_attribution = torch.nn.functional.interpolate(transformer_attribution, scale_factor=16, mode='bilinear')\n",
    "    # transformer_attribution = transformer_attribution.reshape(224, 224).data.cpu().numpy()\n",
    "    transformer_attribution = (transformer_attribution - transformer_attribution.min()) / (transformer_attribution.max() - transformer_attribution.min())\n",
    "\n",
    "    if use_thresholding:\n",
    "      transformer_attribution = transformer_attribution * 255\n",
    "      transformer_attribution = transformer_attribution.astype(np.uint8)\n",
    "      ret, transformer_attribution = cv2.threshold(transformer_attribution, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "      transformer_attribution[transformer_attribution == 255] = 1\n",
    "\n",
    "    image_transformer_attribution = original_image.permute(1, 2, 0).data.cpu().numpy()\n",
    "    image_transformer_attribution = (image_transformer_attribution - image_transformer_attribution.min()) / (image_transformer_attribution.max() - image_transformer_attribution.min())\n",
    "    vis = show_cam_on_image(image_transformer_attribution, transformer_attribution)\n",
    "    vis =  np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    return vis\n",
    "\n",
    "def print_top_classes(predictions, **kwargs):    \n",
    "    # Print Top-5 predictions\n",
    "    prob = torch.softmax(predictions, dim=1)\n",
    "    class_indices = predictions.data.topk(5, dim=1)[1][0].tolist()\n",
    "    max_str_len = 0\n",
    "    class_names = []\n",
    "    for cls_idx in class_indices:\n",
    "        class_names.append(CLS2IDX[cls_idx])\n",
    "        if len(CLS2IDX[cls_idx]) > max_str_len:\n",
    "            max_str_len = len(CLS2IDX[cls_idx])\n",
    "    \n",
    "    print('Top 5 classes:')\n",
    "    for cls_idx in class_indices:\n",
    "        output_string = '\\t{} : {}'.format(cls_idx, CLS2IDX[cls_idx])\n",
    "        output_string += ' ' * (max_str_len - len(CLS2IDX[cls_idx])) + '\\t\\t'\n",
    "        output_string += 'value = {:.3f}\\t prob = {:.1f}%'.format(predictions[0, cls_idx], 100 * prob[0, cls_idx])\n",
    "        print(output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLWPS",
   "language": "python",
   "name": "mlwps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
